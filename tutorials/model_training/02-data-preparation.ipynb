{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a303c34-02dd-49a6-a8a1-e8e6c0538788",
   "metadata": {},
   "source": [
    "## LandCoverNet Data Preparation\n",
    "\n",
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "This tutorial delves into building a scalable model on the LandCoverNet dataset.\n",
    "\n",
    "This portion of the tutorial is focused on developing a semantic segmentation model for LandCoverNet data\n",
    "Here:\n",
    "\n",
    "1. We will inspect the source imagery for the labels we have\n",
    "\n",
    "2. We will process the source imagery in parallel using Dask\n",
    "\n",
    "3. We will select the labels and filtered source images from Dask to be loaded \n",
    "\n",
    "4. We will save the images and associated labels data as a `pickle` file ('.pkl') on our directory to be loaded for model training\n",
    "\n",
    "The esip-summer-2021-geospatial-ml tutorial was helpful in creating this notebook, which can be found [here](https://github.com/TomAugspurger/esip-summer-2021-geospatial-ml/blob/main/segmentation-model.ipynb). It was particularly useful for loading the STAC items and Sentinel-2 scenes using the `stackstac` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7e8ca-7bc6-42b7-9be8-0c00fb1a318c",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "As demonstrated in the [Data Exploration notebook](/1.%20Data%20Exploration.ipynb\") of this tutorial series, access to the Radiant MLHub API using the `pystac_client` library requires both an API end-point and API key. This notebook assumes that you have already followed the steps in `1. Data Exploration.ipynb` and also already have an MLHub API key that is not expired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153b89b-440d-4d9d-a209-e5f37b0a5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "MLHUB_API_KEY = getpass.getpass(prompt=\"MLHub API Key: \")\n",
    "MLHUB_ROOT_URL = \"https://api.radiant.earth/mlhub/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98927dce-f400-4161-8930-fb72cdec4b30",
   "metadata": {},
   "source": [
    "There are a number of STAC and geospatial related libraries used in this notebook that need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03050794-0493-4981-bed3-7a4100120db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pystac\n",
    "import warnings\n",
    "import pystac_client\n",
    "from shapely.geometry import mapping, shape\n",
    "import rioxarray\n",
    "from pystac import Item\n",
    "from typing import List, Tuple\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)  # ignore warnings that get printed out\n",
    "from datetime import datetime\n",
    "\n",
    "import stackstac\n",
    "import rasterio as rio\n",
    "import rasterio.plot\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import dask\n",
    "import dask_gateway\n",
    "from pystac.item_collection import ItemCollection\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7c5db-cc3a-4237-89e9-10e0972e7de8",
   "metadata": {},
   "source": [
    "### Launch a Dask gateway cluster for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b960e1-0660-452f-8a8e-2282438c9ad1",
   "metadata": {},
   "source": [
    "We will use Dask to optimize our data processing of thousands of source image chips by parallelizing the workflow with a delayed computation graph. The Dask Client schedules, runs the delayed computations, and gathers the results, while the Dask Gateway provides a secure and centralized way of managing the multiple client clusters. This is especially useful for running Dask on Planetary Computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680af86-7399-460b-944f-57b6f51e62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client.run(lambda: warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007756f-3df8-461d-a63a-d179eaaef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = dask_gateway.Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options[\"worker_cores\"] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85490b75-fb5c-4789-a880-e05d6c5517b5",
   "metadata": {},
   "source": [
    "### Instantiate an instance of the MLHub API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad7e45-bcf9-428d-b1a0-03efdeec7f37",
   "metadata": {},
   "source": [
    "Here again we demonstrate how to instantiate an API client connected to the MLHub end-point using the `pystac_client` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703906b-fbfb-4a9c-86f0-78273b9ebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlhub_client = pystac_client.Client.open(\n",
    "    url=MLHUB_ROOT_URL, parameters={\"key\": MLHUB_API_KEY}, ignore_conformance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531848a9-6c38-44ea-bbd6-a61aec020df5",
   "metadata": {},
   "source": [
    "We set the temporary directory based on the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64905eb-9dbf-4d77-8010-7d4eb5a950fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = os.path.join(os.getcwd(), \"landcovernet\")\n",
    "labels_dir = os.path.join(tmp_dir, \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a072c3a-69fb-4e69-bd40-f813a307d480",
   "metadata": {},
   "source": [
    "We need to make sure that the labels collection has already been downloaded to the Planetary Computer instance we are running, stored in the shared directory `/home/jovyan/PlanetaryComputerExamples/landcovnet`. Please double-check the active working directory to make sure that the catalog is found when you run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a62dc5-7197-4842-ae8f-bc165169a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for data in collection file\n",
    "catalog = pystac.read_file(\n",
    "    os.path.join(labels_dir, \"ref_landcovernet_v1_labels/collection.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcd2d5-c7a7-4ea6-b3c5-30964152540e",
   "metadata": {},
   "source": [
    "### Loading the source imagery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a399a24-0d45-4049-bb41-e03efa3b0527",
   "metadata": {},
   "source": [
    "In order to fetch the source images from the source Item Assets, first we need to gather all of the label Items from the LandCoverNet labels collection we downloaded in the previous tutorial. This grabs all of the label Item STAC objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6e5cd-e912-4d92-a107-9bb24fa71883",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = catalog.get_item_links()  # links from the catalog\n",
    "label_items = [link.resolve_stac_object().target for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50cd6ce-9c9e-49cd-8900-2fe583bdbbbd",
   "metadata": {},
   "source": [
    "This is a helper function to calculate the percent of a raster image that is covered with clouds. It is assumed that the image input dimensions are 256 by 256 pixels. The sum of cloud cover across the image is normalized and divided by the total area of the chip. This returns an integer value of cloud cover between 0 and 100 to be passed to the STAC Item metadata. ***NOTE: This function is only called if the Item metadata does not include the `eo:cloud_cover` property.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3835b-5c44-489a-b4a2-da953eb031ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cloud_cover(img_arr: np.ndarray) -> int:\n",
    "\n",
    "    \"\"\"Takes a chip cloud cover band and returns the integer score\n",
    "    by dividing the sum of normalized values by the chip area (HxW).\n",
    "\n",
    "    Args:\n",
    "    img_arr: np.ndarray - 2d array of cloud cover mask\n",
    "\n",
    "    Returns:\n",
    "    arr_cc: int - integer value of cloud cover score\n",
    "\n",
    "    \"\"\"\n",
    "    CHIP_AREA = 256 * 256\n",
    "    arr_filled = np.nan_to_num(img_arr)\n",
    "    arr_norm = arr_filled / 100\n",
    "    arr_sum = arr_norm.sum()\n",
    "    arr_cc = arr_sum / CHIP_AREA * 100\n",
    "    return int(arr_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c443c-1a4b-4aa7-97aa-ab14bf168b52",
   "metadata": {},
   "source": [
    "For our use-case, we decided not to train the model on the entire source LandCoverNet dataset. Instead we take chip samples that were representative of each season or quadrimester, or any custom number of bins spread over a temporal range. \n",
    "\n",
    "This is a helper function that returns the median date from a set of all dates in a range representing each source Item linked to a label Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8adef1-2779-407c-8714-41f49f5391cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_date(id_arr: np.ndarray) -> int:\n",
    "\n",
    "    \"\"\"Takes a 2d array of source Item IDs for a quarter, and returns median date\n",
    "\n",
    "    Args: id_arr: np.ndarray - 2d array of string values for source Item IDs\n",
    "\n",
    "    Returns:\n",
    "    median_date: int - the calculated median date value for input array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dates = [int(s[-8:]) for s in id_arr]\n",
    "    dates.sort()\n",
    "\n",
    "    n = len(dates)\n",
    "\n",
    "    # case in which multiple items returned\n",
    "    if n > 1:\n",
    "        if n % 2 == 0:\n",
    "            mid = int(n / 2)\n",
    "        else:\n",
    "            mid = int((n + 1) / 2)\n",
    "        median_date = dates[mid]\n",
    "    # base case there is only one source item\n",
    "    elif n == 1:\n",
    "        median_date = dates[0]\n",
    "    # base case there are no source items\n",
    "    else:\n",
    "        median_date = 0\n",
    "\n",
    "    return median_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b5a0f-4b21-427a-9dc6-f1ddc7099d22",
   "metadata": {},
   "source": [
    "This helper function assigns an integer value from a datetime value based on the `period_ranges` variable created by another function below called `get_date_ranges()`. The period value assigned is used later to group and rank source Items on their cloud cover value so that within each temporal period, we are only working with the images with minimal cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130240e-bbeb-4c89-a322-bbd26815f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_temporal_period(dt: datetime) -> int:\n",
    "    \"\"\"Takes a datetime and returns an integer based on n_periods defined\"\"\"\n",
    "    for ix, pair in enumerate(period_ranges):\n",
    "        if dt >= pair[0] and dt <= pair[1]:\n",
    "            return ix + 1\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f76591-6b05-4427-8431-09ff053688a1",
   "metadata": {},
   "source": [
    "This takes the DataFrame created from Item metadata in `get_season_min_cloud_cover()`, ranks the dates for each period by cloud cover value, and returns a single source chip for each datetime periods the Items are split into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adfdfb-c777-42c1-a4ab-02b5f540dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_period_items(cc_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Takes a dataframe of source Items with metadata and filters\n",
    "    on ranked cloudcover by period/season (quadrimester).\n",
    "\n",
    "    Args:\n",
    "    cc_df: pd.DataFrame - unfiltered dataframe\n",
    "\n",
    "    Returns:\n",
    "    filtered_df: pd.Dataframe - filtered dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    # assigns quarter and rank by quarter\n",
    "    cc_df[\"date_time\"] = pd.to_datetime(cc_df[\"date_time\"])\n",
    "    cc_df[\"period\"] = cc_df[\"date_time\"].apply(assign_temporal_period)\n",
    "    cc_df[\"rank\"] = cc_df.groupby(\"period\")[\"cloud_cover\"].rank(\n",
    "        method=\"min\", ascending=True\n",
    "    )\n",
    "\n",
    "    id_prefix = cc_df.iloc[0][\"id\"][:-8]\n",
    "    median_dates = []\n",
    "\n",
    "    # filters DataFrame on rank\n",
    "    min_cc_df = cc_df[cc_df[\"rank\"] == 1]\n",
    "\n",
    "    # for each quadrimester in year, get the median date of source items\n",
    "    for i in range(1, n_periods + 1):\n",
    "        quarter_df = min_cc_df[min_cc_df[\"period\"] == i]\n",
    "        quarter_median_date = get_median_date(quarter_df[\"id\"].values)\n",
    "        quarter_median_id = id_prefix + str(quarter_median_date)\n",
    "        median_dates.append(quarter_median_id)\n",
    "\n",
    "    # filter the ranked DataFrame by median date\n",
    "    filtered_df = min_cc_df[min_cc_df[\"id\"].isin(median_dates)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17c5f0-91cf-45b2-b0a6-2e3fc26bd5ee",
   "metadata": {},
   "source": [
    "This is a wrapper function that creates a DataFrame from a list of Items, and calls the nested filtering functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d596e-d9af-4c5f-a507-5aff263f293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_min_cloud_cover(item_list: List[Item]) -> ItemCollection:\n",
    "\n",
    "    \"\"\"Takes a list of source Items and returns a single chip per season\n",
    "    ranked by the minimum cloud cover from eo:cloud_cover property\n",
    "\n",
    "    Args:\n",
    "    item_list: List[Item] - iterable of source Items returned from search\n",
    "\n",
    "    Returns:\n",
    "    ItemCollection - STAC Iterable containing Items filtered by cloud cover\n",
    "    \"\"\"\n",
    "\n",
    "    # constructs a DataFrame of each source item properties\n",
    "    df_list = []\n",
    "    for ui in item_list:\n",
    "        if \"eo:cloud_cover\" in ui.properties:\n",
    "            cloud_cover = ui.properties[\"eo:cloud_cover\"]\n",
    "        else:\n",
    "            cloud_cover = calculate_cloud_cover(\n",
    "                rio.open(ui.get_assets()[\"CLD\"].href).read()\n",
    "            )\n",
    "        uid = {\n",
    "            \"item\": ui,\n",
    "            \"id\": ui.id,\n",
    "            \"cloud_cover\": cloud_cover,\n",
    "            \"date_time\": ui.datetime,\n",
    "        }\n",
    "        df_list.append(uid)\n",
    "\n",
    "    cc_df = pd.DataFrame(df_list)\n",
    "\n",
    "    # filters source items by cloud cover rank and returns ItemCollection\n",
    "    if not cc_df.empty:\n",
    "        filtered_df = filter_period_items(cc_df)\n",
    "\n",
    "        return ItemCollection(filtered_df[\"item\"].tolist())\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e5351-f592-45d7-91cd-d3ccda073626",
   "metadata": {},
   "source": [
    "This will take the temporal and spatial extent of an Item to query MLHub API client for matching source Items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ad2c5-0ea6-47a0-afe0-6e305a68720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_item_collection(label_item: Item) -> ItemCollection:\n",
    "\n",
    "    \"\"\"Takes a label Item from the LandCoverNet Collection and searches\n",
    "    for source imagery for chips that match spatial and temporal criteria\n",
    "\n",
    "    Args:\n",
    "    label_item: Item - item of current iteration in the get_item() Dask parallelization\n",
    "\n",
    "    Returns:\n",
    "    ItemCollection - STAC Iterable containing Items that match search criteria\n",
    "    \"\"\"\n",
    "\n",
    "    n = 0\n",
    "    cc_thresh = 10\n",
    "    year_collection = ItemCollection([])\n",
    "\n",
    "    # iterate over each start and end date per quarter\n",
    "    for start, end in period_ranges:\n",
    "\n",
    "        while n == 0:\n",
    "\n",
    "            # performs a temporal and spatial search for each label item\n",
    "            search = mlhub_client.search(\n",
    "                collections=[\"ref_landcovernet_v1_source\"],\n",
    "                intersects=mapping(shape(label_item.geometry)),\n",
    "                datetime=[start, end],\n",
    "                query={\"eo:cloud_cover\": {\"lt\": cc_thresh}},\n",
    "            )\n",
    "\n",
    "            # converts search results to ItemCollection\n",
    "            item_results = search.get_all_items()\n",
    "\n",
    "            if not item_results:\n",
    "                cc_thresh += 5\n",
    "            else:\n",
    "                n = len(item_results)\n",
    "\n",
    "        year_collection += item_results  # concatenate ItemCollections for each quarter\n",
    "        n = 0  # reset the length criteria for search results\n",
    "\n",
    "    filtered_items = get_season_min_cloud_cover(year_collection.items)\n",
    "\n",
    "    return filtered_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a27886-df90-4143-b966-834b2295c702",
   "metadata": {},
   "source": [
    "This is the primary function that drives all the processing required to filter and load source imagery and label data into a stack of Xarray DataArrays for further processing, e.g. splitting the dataset into training and validation sets prior to training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fbbe5-4ed8-4c43-b7b9-2a29bac7c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(label_item: Item, assets: Tuple[str]) -> (np.ndarray, np.ndarray):\n",
    "\n",
    "    \"\"\"Takes label Item and asset bands to construct n-darrays for model training\n",
    "\n",
    "    Args:\n",
    "    label_item: Item - item of current iteration in the get_item() Dask parallelization\n",
    "    assets: Tuple[str] - a set of strings corresponding to the Asset band names\n",
    "\n",
    "    Returns:\n",
    "    data: np.ndarray, labels: np.ndarray - X and y n-darrays for model training\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    assets = list(assets)\n",
    "    labels = rioxarray.open_rasterio(\n",
    "        tmp_dir + \"/labels/ref_landcovernet_v1_labels/\" + label_item.id + \"/labels.tif\",\n",
    "    ).squeeze()\n",
    "\n",
    "    source_item_collection = get_label_item_collection(label_item)\n",
    "\n",
    "    if len(source_item_collection) > 0:\n",
    "\n",
    "        bounds = tuple(round(x, 0) for x in labels.rio.bounds())\n",
    "\n",
    "        data = stackstac.stack(\n",
    "            items=source_item_collection,\n",
    "            assets=assets,\n",
    "            dtype=\"float32\",\n",
    "            resolution=10,\n",
    "            bounds=bounds,\n",
    "            epsg=labels.rio.crs.to_epsg(),\n",
    "        )\n",
    "\n",
    "        data = data.assign_coords(x=labels.x.data, y=labels.y.data)\n",
    "        data /= 4000\n",
    "        data = np.clip(data, 0, 1)\n",
    "\n",
    "        return data, labels.astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a759a7-14f7-4331-a9ea-a7768a986edf",
   "metadata": {},
   "source": [
    "This takes in the temporal range of the Collection as well as a global variable `n_periods` defined below to return a list of datetime ranges split up into equal sized buckets based on the designated number of periods. E.g.  `n_periods=3` will return quadrimesters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173b773-3ce5-4de1-966b-b23243aa6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges(\n",
    "    start: datetime, end: datetime, periods: int\n",
    ") -> List[List[datetime]]:\n",
    "\n",
    "    \"\"\"Builds a list of start and end date ranges for every four in the year\n",
    "\n",
    "    Args: None\n",
    "    Returns:\n",
    "    quarter_ranges: List[List[datetime]] - a list of pairs of strings representing\n",
    "        the start and end dates.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    period_ranges = []\n",
    "    all_dates = pd.DataFrame(\n",
    "        pd.date_range(start=temporal_start, end=temporal_end, freq=\"1D\"),\n",
    "        columns=[\"Date\"],\n",
    "    )\n",
    "    date_bins = pd.cut(all_dates.Date, bins=periods).drop_duplicates()\n",
    "\n",
    "    for interval in date_bins:\n",
    "        period_ranges.append(\n",
    "            [\n",
    "                interval.left.tz_localize(\"UTC\").to_pydatetime(),\n",
    "                interval.right.tz_localize(\"UTC\").to_pydatetime(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return period_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4da8ac-371e-4a11-a5a7-845ca96dee11",
   "metadata": {},
   "source": [
    "Here we specify the temporal extent of the Catalog, n periods to divide the temporal range into, and bands to fetch for each source Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda7a62-cd3a-41e4-ab52-d0ba985b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_start = catalog.extent.temporal.intervals[0][0].strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")  # global starting datetime for label Collection\n",
    "temporal_end = catalog.extent.temporal.intervals[0][1].strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")  # global ending datetime for label Collection\n",
    "n_periods = 5\n",
    "\n",
    "period_ranges = get_date_ranges(temporal_start, temporal_end, n_periods)\n",
    "assets = (\"B04\", \"B03\", \"B02\")  # we will make use of the RGB bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12727fc1-67ed-4ee3-a9ff-87ce643576c3",
   "metadata": {},
   "source": [
    "### Load the source imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec271c06-48be-4dd4-b7e0-98a17cbcddf1",
   "metadata": {},
   "source": [
    "Now we will bring everything together. We setup the `get_item()` function defined above as a Dask delayed function, and append the lazy results from fetching N source images (`n_periods` variable above) for each label Item in the Catalog. Then the actual computation occurs in parallel, and the results are appended to a list of DataArrays containing the aligned images and labels (X and y features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c255ac3-223e-4fc5-a480-1a836865fb72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Xys_list = []\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"distributed.utils_perf\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "chunk_size = 20\n",
    "for i in range(0, len(label_items), chunk_size):\n",
    "    label_chunk = label_items[i : i + chunk_size]\n",
    "\n",
    "    Xys = []\n",
    "    get_item_ = dask.delayed(get_item, nout=5)\n",
    "\n",
    "    Xys.append([get_item_(label, assets) for label in label_chunk])\n",
    "    Xys = dask.persist(*Xys)\n",
    "    Xys = dask.compute(*Xys)\n",
    "    Xys_list.append(Xys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d6a14-fb78-4f4e-a98e-ee3077a26bb2",
   "metadata": {},
   "source": [
    "The Dask client can be shutdown with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb07ea9-8e36-43fd-92d4-0d072aab1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe1876-dc80-4cdc-85a1-cf06ba2ca576",
   "metadata": {},
   "source": [
    "We stacked the results of our parallelized function into chunks of 20 Items at a time, so this will flatten the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d6014-40e8-4441-9b80-9905d3dd5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in Xys_list for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b2c15-3536-4d7f-9c1f-59809dc1504e",
   "metadata": {},
   "source": [
    "This confirms that every item was extracted, e.g. the flattened list has the length of label Items fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f19675-5f2e-4bbd-89ae-0c565ba92a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_list)  # confirm every item was extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28482cc9-c091-4286-8aa0-53b0907a1ed1",
   "metadata": {},
   "source": [
    "This confirms that the shape of data for each label item has n source items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ce4a4-80ee-4262-8e0f-2fd1afe42047",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list[0][0].shape  # confirm that we have the desired shape for a chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bccfdf-8553-4a34-a6c0-252df4dff865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf labels #clear up labels to clear PC space. may choose to leave the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d433db3-a018-48b6-8a4e-022b7a47f979",
   "metadata": {},
   "source": [
    "The last step before training a neural network is to dump the image dataset we just created into a pickle file stored locally on the Planetary Computer instance running. This is an efficient way to store and load the dataset in the next notebook and to conserve memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e349c3-3ece-4fd6-b800-5ac360dd874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((flat_list), open(f\"{tmp_dir}/items\" + \".pkl\", \"ab\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
